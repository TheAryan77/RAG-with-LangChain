# ğŸ“š RAG Pipeline - Retrieval-Augmented Generation System

A production-ready RAG (Retrieval-Augmented Generation) pipeline for intelligent document question-answering using LangChain, ChromaDB, and Groq LLM.

## ğŸŒŸ Features

- **PDF Document Processing**: Automatically load and process PDF files from directories
- **Intelligent Text Chunking**: Split documents into optimal chunks for better retrieval
- **Vector Storage**: ChromaDB-powered persistent vector database with cosine similarity
- **Semantic Search**: Sentence-Transformers embeddings for accurate document retrieval
- **Multiple RAG Pipelines**:
  - Simple RAG: Basic question-answering
  - Enhanced RAG: With confidence scores and source citations
  - Advanced RAG: Streaming, query history, and answer summarization
- **LLM Integration**: Groq API for fast, high-quality answer generation

## ğŸ—ï¸ Project Structure

```
RAG/
â”œâ”€â”€ data/                      # Data storage
â”‚   â”œâ”€â”€ text_files/           # Text documents
â”‚   â””â”€â”€ vector_store/         # ChromaDB persisted vectors
â”œâ”€â”€ notebooks/                 # Jupyter notebooks for experimentation
â”‚   â”œâ”€â”€ document.ipynb        # Document loading examples
â”‚   â”œâ”€â”€ pdf_loader.ipynb      # PDF processing
â”‚   â””â”€â”€ rag_pipeline.ipynb    # Complete RAG pipeline
â”œâ”€â”€ src/                       # Source code modules
â”‚   â”œâ”€â”€ data_loader.py        # Document loading utilities
â”‚   â”œâ”€â”€ embeddings.py         # Embedding generation
â”‚   â”œâ”€â”€ vectorstore.py        # Vector database management
â”‚   â””â”€â”€ search.py             # Retrieval and search logic
â”œâ”€â”€ main.py                    # Main application entry point
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ pyproject.toml            # Project configuration
â””â”€â”€ .env                       # Environment variables (not tracked)
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Git
- Groq API Key ([Get one here](https://console.groq.com))

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd RAG
   ```

2. **Create virtual environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   
   Or using `uv` (faster):
   ```bash
   uv pip install -r requirements.txt
   ```

4. **Set up environment variables**
   
   Create a `.env` file in the root directory:
   ```env
   GROQ_API_KEY=your_groq_api_key_here
   ```

### Usage

#### 1. Basic Python Script

```python
from src.data_loader import process_all_pdfs
from src.embeddings import EmbeddingManager
from src.vectorstore import VectorStore
from src.search import RAGRetriever

# Load documents
documents = process_all_pdfs("./data")

# Generate embeddings
embedding_manager = EmbeddingManager()
embeddings = embedding_manager.generate_embeddings([doc.page_content for doc in documents])

# Store in vector database
vectorstore = VectorStore()
vectorstore.add_documents(documents, embeddings)

# Query
retriever = RAGRetriever(vectorstore, embedding_manager)
results = retriever.retrieve("What is ISO?", top_k=3)
```

#### 2. Using Jupyter Notebooks

Launch Jupyter and explore the notebooks:
```bash
jupyter notebook notebooks/rag_pipeline.ipynb
```

The notebook contains:
- Complete RAG pipeline setup
- PDF processing examples
- Simple, Enhanced, and Advanced RAG implementations
- Interactive query testing

## ğŸ“‹ Components

### Data Loader
Processes PDF files from directories and extracts text with metadata.

### Embedding Manager
Uses `sentence-transformers` (all-MiniLM-L6-v2) to generate 384-dimensional embeddings.

### Vector Store
ChromaDB-based persistent storage with cosine similarity for efficient retrieval.

### RAG Retriever
Handles query processing and returns ranked, relevant document chunks.

### RAG Pipelines

1. **Simple RAG**: Basic retrieval + LLM generation
2. **Enhanced RAG**: Adds confidence scores and source attribution
3. **Advanced RAG**: Query history, streaming, and summarization

## ğŸ”§ Configuration

### Embedding Model
Change the embedding model in `EmbeddingManager`:
```python
embedding_manager = EmbeddingManager(model_name="all-mpnet-base-v2")
```

### Chunking Strategy
Adjust chunk size and overlap in text splitting:
```python
split_documents(documents, chunk_size=1000, chunk_overlap=200)
```

### LLM Model
Configure Groq model:
```python
llm = ChatGroq(
    groq_api_key=api_key,
    model_name="openai/gpt-oss-120b",  # or other Groq models
    temperature=0.1,
    max_tokens=1024
)
```

## ğŸ“Š Example Output

```python
query = "What is ISO?"
result = rag_advanced(query, rag_retriever, llm, top_k=3)

# Output:
{
    'answer': 'ISO stands for International Organization for Standardization...',
    'sources': [
        {'source': 'document.pdf', 'page': 5, 'score': 0.89},
        {'source': 'guide.pdf', 'page': 12, 'score': 0.85}
    ],
    'confidence': 0.89
}
```

## ğŸ› ï¸ Development

### Running Tests
```bash
pytest tests/
```

### Code Formatting
```bash
black src/
ruff check src/
```

## ğŸ“¦ Dependencies

- **LangChain**: Document processing and RAG orchestration
- **ChromaDB**: Vector database for embeddings
- **Sentence Transformers**: Embedding generation
- **PyPDF/PyMuPDF**: PDF parsing
- **Groq**: LLM API for answer generation
- **Python-dotenv**: Environment variable management

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- LangChain for the RAG framework
- ChromaDB for vector storage
- Groq for fast LLM inference
- Sentence Transformers for embeddings

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with â¤ï¸ for intelligent document search and question-answering**